{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup lib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import requests\n",
    "from os import path,scandir\n",
    "import datetime\n",
    "\n",
    "import dpkt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import tarfile\n",
    "import logging\n",
    "import pickle\n",
    "from tensorboardX import SummaryWriter\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logger\n",
    "def logging_start():\n",
    "# create logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    \n",
    "# create console handler and set level to debug\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# create formatter\n",
    "    formatter = logging.Formatter('%(asctime)s: %(levelname)s: %(message)s')\n",
    "\n",
    "# add formatter to ch\n",
    "    ch.setFormatter(formatter)\n",
    "                                  \n",
    "# add ch to logger\n",
    "    logger.addHandler(ch)\n",
    "    \n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Pre-prossesing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_preporation():\n",
    "    def __init__(self,data,batch_size):\n",
    "        self.data=data\n",
    "        self.batch_size=batch_size\n",
    "        self.pointer = 0\n",
    "\n",
    "        self.data=self.data[:-1000]\n",
    "        self.test_data=self.data[-1000:]\n",
    "        self.batch_creation()\n",
    "\n",
    "    def batch_creation(self):\n",
    "        self.num_batches = self.data.shape[0] // self.batch_size\n",
    "        #print(self.num_batches)\n",
    "        self.x_train = self.data[:self.num_batches * self.batch_size]\n",
    "        self.x_batches = np.split(self.x_train, self.num_batches)\n",
    "        #print(x_train.shape)\n",
    "        #print(x_batches)\n",
    "        \n",
    "    def next_batch(self):\n",
    "        #x,y = db.next_batch()\n",
    "        self.x_batch = self.x_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        # #x_test, y_test = self.x_test_batches[self.pointer], self.y_test_batches[self.pointer]\n",
    "        # return x, y, self.x_test, self.y_test\n",
    "        return self.x_batch,self.test_data\n",
    "    \n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0\n",
    "        \n",
    "    def get_num_batches(self):\n",
    "        return self.num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_bitstream(num_packets_limit=None,num_databytes=None,new_number_data_bytes=False, pcap_file=\"F:\\\\DARPA1999\\\\Week1\\\\training.pcap\"):\n",
    "    if num_databytes is None:\n",
    "        packet_size = None\n",
    "    else:\n",
    "        packet_size = 304 + (num_databytes*8)\n",
    "\n",
    "    if path.isfile(\"F:\\\\DARPA1999\\\\Week1\\\\preprocessed_bits.pkl\") and new_number_data_bytes==False:\n",
    "        logging.info(\"Using saved processed data...\")\n",
    "        trimmed_bits_numpy = np.load(open(\"F:\\\\DARPA1999\\\\Week1\\\\preprocessed_bits.pkl\",\"rb\"),allow_pickle=True)\n",
    "        return trimmed_bits_numpy,trimmed_bits_numpy.shape[1]\n",
    "    elif (path.isfile(\"F:\\\\DARPA1999\\\\Week1\\\\untrimmed_bytes.pkl\")):\n",
    "        logging.info(\"Using saved untrimmed processed data...\")\n",
    "        untrimmed_bytes_list = np.load(open(\"F:\\\\DARPA1999\\\\Week1\\\\untrimmed_bytes.pkl\",\"rb\"),allow_pickle=True)\n",
    "    else: \n",
    "        logger.info(\"Reading Packets from pcap file %s\" % pcap_file)\n",
    "        pcap = dpkt.pcap.Reader(open(pcap_file, 'rb'))\n",
    "        TCP_packets = []\n",
    "        count = 0\n",
    "            \n",
    "            \n",
    "        untrimmed_bytes_list=[]\n",
    "        for ts, buf in tqdm(pcap):\n",
    "            bit_string=\"\"\n",
    "            # print(type(buf))\n",
    "            for bytes in list(buf):\n",
    "                bit_string += '{0:08b}'.format(bytes)\n",
    "                \n",
    "            #print(\"packet size\",len(bit_string))\n",
    "            untrimmed_bytes_list.append(bit_string)\n",
    "            count+=1\n",
    "\n",
    "            if count == num_packets_limit and num_packets_limit is not None:\n",
    "                break\n",
    "                    \n",
    "        pickle.dump(untrimmed_bytes_list, open(\"F:\\\\DARPA1999\\\\Week1\\\\untrimmed_bytes.pkl\", \"wb\"), protocol=4)\n",
    "        \n",
    "    max_bytes = int(len(max(untrimmed_bytes_list,key=len)))\n",
    "        \n",
    "    if packet_size is None:\n",
    "        packet_size=max_bytes\n",
    "            \n",
    "    #print(packet_size > max_bytes)\n",
    "    #print()\n",
    "    assert packet_size <= max_bytes, \"The packets size cant extend maximum bytes, found %s > %s\"% (packet_size,max_bytes,)\n",
    "        \n",
    "    print(\"\\rLimit on Number of Packets: {}, Limit on Data Bytes: {}, Packet Size:{}\".format(num_packets_limit, num_databytes,packet_size))\n",
    "    \n",
    "    #logging.info()\n",
    "    trimmed_bits_list=[sublist[:packet_size] for sublist in untrimmed_bytes_list]\n",
    "        \n",
    "    #print(trimmed_bits_list[:2])\n",
    "    #print(len(max(trimmed_bits_list, key=len)))\n",
    "    #print(len(trimmed_bits_list[0]))\n",
    "    #print((untrimmed_bytes_list[0]))\n",
    "        \n",
    "    trimmed_bits_list = [[(x + \"0\" * (packet_size - len(x)))] for x in tqdm(trimmed_bits_list)]\n",
    "    #print(trimmed_bits_list)\n",
    "    \n",
    "    #print(trimmed_bits_list[:2])\n",
    "    assert len(trimmed_bits_list) == len(untrimmed_bytes_list), \"Trimmed bits must be the same len as untrimmed, \" \\\n",
    "        \"found %s > %s\" % (len(trimmed_bits_list), len(untrimmed_bytes_list),)\n",
    "        \n",
    "    #print(len(max(trimmed_bits_list, key=len)))\n",
    "    #print(len(max(trimmed_bits_list, key=len)))\n",
    "    trimmed_bits_numpy = np.asanyarray([list(map(int,string)) for lists in tqdm(trimmed_bits_list) for string in lists])\n",
    "        \n",
    "    # print(trimmed_bits_numpy)\n",
    "\n",
    "    assert trimmed_bits_numpy.shape == (num_packets_limit,packet_size) or num_packets_limit == None, \"\\\n",
    "        Trimmed data dont match origanal data, found %s > %s\" % (trimmed_bits_numpy.shape, (num_packets_limit,packet_size)) \n",
    "        #trimmed_bits_numpy.dump(\"./data/preprocessed_bits.pkl\")\n",
    "    pickle.dump(trimmed_bits_numpy,open(\"F:\\\\DARPA1999\\\\Week1\\\\preprocessed_bits.pkl\",\"wb\"),protocol=4)\n",
    "    \n",
    "    #print the first 100 values of first packet\n",
    "    #print(\"The first 100 values of first packet: \",trimmed_bits_numpy[0:1,0:100])\n",
    "    return trimmed_bits_numpy,trimmed_bits_numpy.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using saved processed data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12112\n"
     ]
    }
   ],
   "source": [
    "bitstream, packet_size = data_to_bitstream(num_packets_limit=None,num_databytes=None,new_number_data_bytes=False, pcap_file=\"F:\\\\DARPA1999\\\\Week1\\\\training.pcap\")\n",
    "print(packet_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attack_packets(packet_size):\n",
    "    if path.isfile(\"F:\\\\DARPA1999\\\\Week1\\\\preprocessed_attacks.pkl\"):\n",
    "        logging.info(\"Using saved processed data...\")\n",
    "        trimmed_bits_numpy = np.load(open(\"F:\\\\DARPA1999\\\\Week1\\\\preprocessed_attacks.pkl\", \"rb\"), allow_pickle=True)\n",
    "        return trimmed_bits_numpy,trimmed_bits_numpy.shape[1]\n",
    "    else:\n",
    "        pcap = dpkt.pcap.Reader(open(\"F:\\\\DARPA1999\\\\Week1\\\\inside.pcap\",'rb'))\n",
    "        attack_list = []\n",
    "        \n",
    "        df = pd.DataFrame(pd.read_csv(open(\"F:\\\\DARPA1999\\\\Week1\\\\attacks.csv\",'r'), sep=\";\", encoding = 'utf-8'))\n",
    "        attack_list.extend([\"\".join(x[1].split(\":\")) for x in df[\"StartTime\"].iteritems()])\n",
    "        \n",
    "        print(attack_list)\n",
    "        print(len(attack_list))\n",
    "        \n",
    "        untrimmed_bytes_list = []\n",
    "        count = 0\n",
    "\n",
    "        attack_packets = []\n",
    "        \n",
    "        for ts, buf in tqdm(pcap):\n",
    "            ts = str(datetime.datetime.utcfromtimestamp(ts) - datetime.timedelta(hours=5))\n",
    "            \n",
    "            date, time = str(ts)[:10], str(ts)[11:]\n",
    "            # print(date,time)\n",
    "\n",
    "            time_long_string = \"\".join(time.split(\":\"))\n",
    "\n",
    "            # print(time_long_string[:6],attack_list)\n",
    "            if time_long_string[:6] in attack_list:\n",
    "                bit_string = \"\"\n",
    "                \n",
    "                # print(\"Found attack\")\n",
    "                # eth=dpkt.ethernet.Ethernet(buf)\n",
    "                # print(repr(eth))\n",
    "                for bytes in list(buf):\n",
    "                    # print(bytes)\n",
    "                    bit_string += '{0:08b}'.format(bytes)\n",
    "                    \n",
    "                    # print(bit_string)\n",
    "                    # break\n",
    "                attack_packets.append(bit_string)\n",
    "                # print(attack_packets)\n",
    "                # print(\"%s Attacks found\" % count)\n",
    "            count += 1\n",
    "            \n",
    "            if time_long_string[:6] == attack_list[-1]:\n",
    "                break\n",
    "                \n",
    "                # if count >= 10000:\n",
    "                # break\n",
    "        trimmed_bits_list = [sublist[:packet_size] for sublist in attack_packets]\n",
    "        \n",
    "        # print(trimmed_bits_list)\n",
    "        trimmed_bits_list = [[(x + \"0\" * (packet_size - len(x)))] for x in tqdm(trimmed_bits_list)]\n",
    "        \n",
    "        # print(trimmed_bits_list[:2])\n",
    "        trimmed_bits_numpy = np.asanyarray([list(map(int, string)) for lists in tqdm(trimmed_bits_list) for string in lists])\n",
    "        \n",
    "        # print(trimmed_bits_numpy)\n",
    "        pickle.dump(trimmed_bits_numpy, open(\"F:\\\\DARPA1999\\\\Week1\\\\preprocessed_attacks.pkl\", \"wb\"), protocol=4)\n",
    "        \n",
    "        print(\"The first 100 values of first packet: \",trimmed_bits_numpy[0:1,0:100])\n",
    "        \n",
    "        return trimmed_bits_numpy,trimmed_bits_numpy.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using saved processed data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]),\n",
       " 12112)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_attack_packets(packet_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_attacks(packet_size):\n",
    "    if path.isfile(\"F:\\\\DARPA1999\\\\Week1\\\\week_4.pkl\"):\n",
    "        # logging.info(\"Using saved processed data...\")\n",
    "        trimmed_bits_numpy = np.load(open(\"F:\\\\DARPA1999\\\\Week1\\\\week_4.pkl\", \"rb\"), allow_pickle=True)\n",
    "        return trimmed_bits_numpy\n",
    "    else:\n",
    "        df = pd.DataFrame(pd.read_csv(open(\"F:\\\\DARPA1999\\\\Week1\\\\attacks.csv\", \"r\",encoding=\"utf-8\"), sep=\";\", encoding=\"UTF-8\"))\n",
    "        #print(df.head())\n",
    "        #print(df[\"id\"])\n",
    "        match_dict={}\n",
    "        for entry in scandir(\"F:\\\\DARPA1999\\\\Week1\"):\n",
    "            attack_list=[]\n",
    "           \n",
    "            if entry.is_file() and entry.name is not None:\n",
    "                #pcap = dpkt.pcap.Reader(open(\"./data/week_4.pcap\", 'rb'))\n",
    "                name= entry.name[:2]\n",
    "                #print()\n",
    "                test =[tuple(str(y).split(\".\")) for x,y in df[\"IDnum\"].iteritems()]\n",
    "                # print(test)\n",
    "                for x in test:\n",
    "                    # print(x[0],name)\n",
    "                    if x[0] == name:\n",
    "                        #print(\"match\")\n",
    "                       \n",
    "                        # print(x[1])\n",
    "                        attack_list.append(x[1])\n",
    "                       \n",
    "                        #print(attack_list)\n",
    "            match_dict[entry.name[:2]]=attack_list\n",
    "           \n",
    "        attack_packets = []\n",
    "        for k,v in match_dict.items():\n",
    "                pcap = dpkt.pcap.Reader(open(\"F:\\\\DARPA1999\\\\Week1\\\\%s.pcap\"%k,'rb'))\n",
    "               \n",
    "                attack_list = v\n",
    "                #print(v)\n",
    "                untrimmed_bytes_list = []\n",
    "                count = 0\n",
    "               \n",
    "                # for ts, buf in tqdm(pcap):\n",
    "                for ts, buf in tqdm(pcap):\n",
    "                   \n",
    "                    if not attack_list:\n",
    "                        break\n",
    "                   \n",
    "                    else:\n",
    "                        ts = str(datetime.datetime.utcfromtimestamp(ts) - datetime.timedelta(hours=5))\n",
    "                       \n",
    "                        date, time = str(ts)[:10], str(ts)[11:]\n",
    "                       \n",
    "                        # print(date,time)\n",
    "                        time_long_string = \"\".join(time.split(\":\"))\n",
    "                       \n",
    "                        #print(time_long_string[:6], attack_list)\n",
    "                       \n",
    "                        if time_long_string[:6] in attack_list:\n",
    "                            bit_string = \"\"\n",
    "                            print(\"Found\")\n",
    "                            # attack_list.pop()\n",
    "                            #print(attack_list)\n",
    "                            for bytes in list(buf):\n",
    "                                #print(bytes)\n",
    "                                bit_string += '{0:08b}'.format(bytes)\n",
    "                               \n",
    "                                #print(bit_string)\n",
    "                                #break\n",
    "                                attack_packets.append(bit_string)\n",
    "                                attack_list.pop(0) if attack_list else None    \n",
    "               \n",
    "                               \n",
    "        trimmed_bits_list = [sublist[:packet_size] for sublist in attack_packets]\n",
    "       \n",
    "        # print(trimmed_bits_list)\n",
    "        trimmed_bits_list = [[(x + \"0\" * (packet_size - len(x)))] for x in tqdm(trimmed_bits_list)]\n",
    "       \n",
    "        # print(trimmed_bits_list[:2])\n",
    "        trimmed_bits_numpy = np.asanyarray([list(map(int, string)) for lists in tqdm(trimmed_bits_list) for string in lists])\n",
    "       \n",
    "        # print(trimmed_bits_numpy)\n",
    "        pickle.dump(trimmed_bits_numpy, open(\"F:\\\\DARPA1999\\\\Week1\\\\attacks.pklweek_4.pkl\", \"wb\"), protocol=4)\n",
    "       \n",
    "        return trimmed_bits_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_attacks(packet_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator_input_sampler():\n",
    "    return lambda m, n: torch.rand(m, n)\n",
    "\n",
    "def write_bytes_to_pcap(list_of_bytes,file_path=\"./data/gan_packets.pcap\"):\n",
    "    logging.info(\"Writing %s packets to file\"%len(list_of_bytes))\n",
    "    \n",
    "    with open(file_path, \"wb\") as f:\n",
    "        fd= dpkt.pcap.Writer(f)\n",
    "        for packet in tqdm(list_of_bytes):\n",
    "            #print(packet)\n",
    "            fd.writepkt(packet, time.time())\n",
    "            #f.flush()\n",
    "\n",
    "def metric(d_attack,zeroes):\n",
    "    if zeroes:\n",
    "        d_attack = np.around(np.sum(np.equal((np.around(d_attack.cpu().data.numpy())),np.zeros_like(d_attack.cpu().data.numpy()))\n",
    "                    / len(d_attack.cpu().data.numpy())),6)\n",
    "    else:\n",
    "        d_attack = np.around(np.sum(np.equal((np.around(d_attack.cpu().data.numpy())),np.ones_like(d_attack.cpu().data.numpy()))\n",
    "                    / len(d_attack.cpu().data.numpy())),)\n",
    "    return d_attack\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
